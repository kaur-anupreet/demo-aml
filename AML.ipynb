{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walgreens Boots AML & Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "azureml.core.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "These cells should be run in advance and not be visible during the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants\n",
    "subscription_id = '' \n",
    "resource_group  = ''\n",
    "workspace_name  = ''\n",
    "experiment_name = 'walgreens-boots-propensity'\n",
    "cluster_name = 'cpucluster'\n",
    "project_folder = 'scripts'\n",
    "\n",
    "PRODUCT_CATEGORIES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key open source data analysis packages\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(color_codes='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directories\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/get_data.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def get_data():\n",
    "    df = pd.read_csv('/tmp/azureml_runs/boots/data-latest.csv')\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df['BOUGHT_CATEGORY_FNN'].values)\n",
    "    y = le.transform(df['BOUGHT_CATEGORY_FNN'].values)\n",
    "\n",
    "    df = df.drop(['BOUGHT_CATEGORY_FNN'], axis=1)\n",
    "\n",
    "    return { \"X\" : df, \"y\" : y }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/register.py\n",
    "\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Run\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_name',\n",
    "        type=str,\n",
    "        default='',\n",
    "        help='Variant name you want to give to the model.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--model_path',\n",
    "        type=str,\n",
    "        default='outputs',\n",
    "        help='Location of trained model.'\n",
    "    )\n",
    "\n",
    "    args, unparsed = parser.parse_known_args()\n",
    "    print(args.model_name)\n",
    "    print(args.model_path)\n",
    "    \n",
    "    run = Run.get_context()\n",
    "    ws = run.experiment.workspace\n",
    "    \n",
    "    tags = {\n",
    "        \"runId\": str(run.id)\n",
    "    }\n",
    "\n",
    "    print(json.dumps(tags))\n",
    "\n",
    "    model = Model.register(ws, model_name = args.model_name, model_path = args.model_path, tags=tags)\n",
    "\n",
    "    print('Model registered: {} \\nModel Description: {} \\nModel Version: {}'.format(model.name, model.description, model.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in to analyze\n",
    "df = pd.read_csv('./data-latest.csv')\n",
    "\n",
    "# Re-order columns for demo\n",
    "props = list(filter(lambda c: not c.startswith('BOUGHT') and re.match(r'CATEGORY_\\d+', c) == None, df.columns))\n",
    "value = list(filter(lambda c: re.match(r'CATEGORY_\\d+', c) != None, df.columns))\n",
    "df = df[props + value + ['BOUGHT_CATEGORY_FNN']]\n",
    "df = df[df['BOUGHT_CATEGORY_FNN'] != 'U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to Azure Machine Learning\n",
    "from azureml.core import Run\n",
    "from azureml.core.compute import AksCompute, ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.runconfig import DataReferenceConfiguration, RunConfiguration\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PublishedPipeline, PipelineRun, Schedule, TrainingOutput\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.train.automl import AutoMLConfig, AutoMLStep\n",
    "from azureml.train.automl.runtime.automlexplainer import retrieve_model_explanation\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "import azureml\n",
    "\n",
    "# Connect to Azure Machine Learning\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "except:\n",
    "    ws = Workspace(subscription_id = subscription_id,\n",
    "                   resource_group = resource_group,\n",
    "                   workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    \n",
    "    print('Workspace config file written')\n",
    "    \n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(data=output, index=['']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup previously configured schedules\n",
    "schedules = Schedule.list(ws)\n",
    "for s in schedules:\n",
    "    s.disable(wait_for_provisioning=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview available columns\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of ages in the dataset\n",
    "sns.distplot(df[['AGE']], bins=[10,20,30,40,50,60,70,80,90,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze distribution of spend in category #1\n",
    "sns.distplot(df[('CATEGORY_FNN_SPEND')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how age influences whether customers have responded to category #1 campaigns\n",
    "g = sns.FacetGrid(df, col='BOUGHT_CATEGORY_FNN')\n",
    "g.map(sns.distplot, 'AGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how gender influences whether customers have responded to category #1 campaigns\n",
    "g = sns.FacetGrid(df, col='BOUGHT_CATEGORY_FNN')\n",
    "g.map(sns.countplot, 'GENDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how age and category #1 & #2 spend influences responding to category #1 campaigns\n",
    "sns.pairplot(df[['AGE', 'CATEGORY_FNN_SPEND', 'CATEGORY_WLN_SPEND', 'BOUGHT_CATEGORY_FNN']], hue='BOUGHT_CATEGORY_FNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup our AML environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provision a compute target\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS12_V2',\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=12)\n",
    "\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "compute_target.status.serialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload our data\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload_files(['./data-latest.csv'], target_path = 'boots', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                                path_on_compute='/tmp/azureml_runs',\n",
    "                                path_on_datastore='boots',\n",
    "                                mode='download',\n",
    "                                overwrite=False)\n",
    "\n",
    "# Create the RunConfiguration object, responsible for the configuration of the execution environment\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "run_config.target = compute_target\n",
    "run_config.data_references = {ds.name: dr}\n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             iterations = 25,\n",
    "                             iteration_timeout_minutes = 5, \n",
    "                             max_cores_per_iteration = 4,\n",
    "                             max_concurrent_iterations = 12,\n",
    "                             primary_metric = 'accuracy',\n",
    "                             data_script = project_folder + '/get_data.py',\n",
    "                             run_configuration = run_config,\n",
    "                             path = project_folder,\n",
    "                             model_explainability = True,\n",
    "                             n_cross_validations = 2,\n",
    "                             preprocess = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=False)\n",
    "remote_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily explore results using interactive widgets\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatically find the best model based on different metrics\n",
    "lookup_metric = 'accuracy'\n",
    "best_run, fitted_model = remote_run.get_output(metric = lookup_metric)\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run explanation data\n",
    "shape_values, expected_values, overall_summary, overall_imp, per_class_summary, per_class_imp = \\\n",
    "    retrieve_model_explanation(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.DataFrame([per_class_summary[2]], columns = per_class_imp[2], index = ['Importance'])\n",
    "with sns.plotting_context('notebook', font_scale=1.4):\n",
    "    plt.subplots(figsize=(13,9))\n",
    "    sns.barplot(data=feat.iloc[:10,:10], orient='h').set_title('Key factors for purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the preferred model for your team to use\n",
    "model = best_run.register_model(model_name = 'category_fnn_model.pkl',\n",
    "                                model_path = 'outputs/model.pkl',\n",
    "                                tags = {'area': 'CATEGORY FNN', 'type': 'classification'})\n",
    "print(model.name, model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Captures training code, dataset, and run when stored\n",
    "model.serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key open source data analysis packages\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(color_codes='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working directories\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)\n",
    "\n",
    "# Data functions\n",
    "def get_cat(index, lower=False):\n",
    "    if index == 0:\n",
    "        cat = 'FNN'\n",
    "        cat = cat if not lower else cat.lower()\n",
    "    elif index == 1:\n",
    "        cat = 'WLN'\n",
    "        cat = cat if not lower else cat.lower()\n",
    "    else:\n",
    "        cat = index + 1\n",
    "    \n",
    "    return cat\n",
    "\n",
    "def prep_data_file(index):\n",
    "    with open('{}/get_data.py'.format(project_folder)) as f:\n",
    "        content = f.read()\n",
    "\n",
    "    cat = get_cat(index)\n",
    "    content = content.replace('BOUGHT_CATEGORY_FNN', 'BOUGHT_CATEGORY_{}'.format(cat))\n",
    "    \n",
    "    cat = get_cat(index, lower=True)\n",
    "    cat_folder = '{}/c_{}'.format(project_folder, cat)\n",
    "    if not os.path.exists(cat_folder):\n",
    "        os.makedirs(cat_folder)\n",
    "\n",
    "    with open('{}/get_data.py'.format(cat_folder), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Generate experiment scripts\n",
    "for i in range(PRODUCT_CATEGORIES):\n",
    "    prep_data_file(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in to analyze\n",
    "df = pd.read_csv('./data-latest.csv')\n",
    "\n",
    "# Re-order columns for demo\n",
    "props = list(filter(lambda c: not c.startswith('BOUGHT') and re.match(r'CATEGORY_\\d+', c) == None, df.columns))\n",
    "value = list(filter(lambda c: re.match(r'CATEGORY_\\d+', c) != None, df.columns))\n",
    "df = df[props + value + ['BOUGHT_CATEGORY_FNN']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup previously configured schedules\n",
    "schedules = Schedule.list(ws)\n",
    "for s in schedules:\n",
    "    s.disable(wait_for_provisioning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wbademobuild0187399067.blob.core.windows.net/images/Pipelines.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use our experiment configuration\n",
    "input_data = DataReference(datastore=ds, \n",
    "                           data_reference_name='training_data',\n",
    "                           path_on_datastore='boots',\n",
    "                           mode='download',\n",
    "                           path_on_compute='/tmp/azureml_runs',\n",
    "                           overwrite=True)\n",
    "\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "run_config.target = compute_target\n",
    "run_config.environment.docker.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a pipeline\n",
    "steps = []\n",
    "current = None\n",
    "\n",
    "# Build a model for every category\n",
    "for i in range(PRODUCT_CATEGORIES):\n",
    "    cat = get_cat(i, lower=True)\n",
    "    \n",
    "    # These are the two outputs from AutoML\n",
    "    metrics_data = PipelineData(name='metrics_data_category_{}'.format(cat),\n",
    "                                datastore=ds,\n",
    "                                pipeline_output_name='metrics_output_category_{}'.format(cat),\n",
    "                                training_output=TrainingOutput(type='Metrics'))\n",
    "\n",
    "    model_data = PipelineData(name='model_data_category_{}'.format(cat),\n",
    "                              datastore=ds,\n",
    "                              pipeline_output_name='best_model_output_category_{}'.format(cat),\n",
    "                              training_output=TrainingOutput(type='Model'))\n",
    "\n",
    "    # AutoML config (note different data files for each model so it's not shared)\n",
    "    automl_config = AutoMLConfig(task = 'classification',\n",
    "                                 iterations = 25,\n",
    "                                 iteration_timeout_minutes = 5, \n",
    "                                 max_cores_per_iteration = 2,\n",
    "                                 max_concurrent_iterations = 8,\n",
    "                                 primary_metric = 'accuracy',\n",
    "                                 data_script = '{}/c_{}/get_data.py'.format(project_folder, cat),\n",
    "                                 run_configuration = run_config,\n",
    "                                 compute_target = compute_target,\n",
    "                                 path = project_folder,\n",
    "                                 n_cross_validations = 2,\n",
    "                                 preprocess = True)\n",
    "    \n",
    "    # AutoML action\n",
    "    automl_step = AutoMLStep(name='automl_module_category_{}'.format(cat),\n",
    "                             automl_config=automl_config,\n",
    "                             inputs=[input_data],\n",
    "                             outputs=[metrics_data, model_data],\n",
    "                             allow_reuse=False)\n",
    "    \n",
    "    # Custom script action to register the model afterwards\n",
    "    register_step = PythonScriptStep(name='register_category_{}'.format(cat),\n",
    "                                     script_name='register.py',\n",
    "                                     compute_target=compute_target,\n",
    "                                     source_directory=project_folder,\n",
    "                                     arguments=['--model_name', 'category_{}_model.pkl'.format(cat), '--model_path', model_data],\n",
    "                                     inputs=[model_data],\n",
    "                                     allow_reuse=False)\n",
    "    \n",
    "    # And chain them together so they run sequentially\n",
    "    if current:\n",
    "        automl_step.run_after(current)\n",
    "\n",
    "    current = register_step\n",
    "\n",
    "    steps.append(automl_step)\n",
    "    steps.append(register_step)\n",
    "\n",
    "pipeline = Pipeline(description='Generate recommendation models',\n",
    "                    workspace=ws,\n",
    "                    steps=steps)\n",
    "\n",
    "pipeline.validate()\n",
    "\n",
    "# Once published, we can invoke on demand via the SDK or via a REST endpoint\n",
    "published_pipeline = pipeline.publish(name='category-based-propensity-pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically run our pipeline when the data changes\n",
    "schedule = Schedule.create(workspace=ws,\n",
    "                           name='category-based-propensity-schedule',\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name='category-based-propensity-schedule',\n",
    "                           datastore=ds,\n",
    "                           path_on_datastore='boots',\n",
    "                           wait_for_provisioning=True,\n",
    "                           polling_interval=1,\n",
    "                           description='Scheduled run of category-based-propensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, run it on demand\n",
    "published_pipeline.submit(ws, published_pipeline.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
